import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from googleapiclient.discovery import build
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import re
from collections import Counter
from youtube_comment_downloader import YoutubeCommentDownloader
import gdown
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from arabert.preprocess import ArabertPreprocessor
from pyarabic.araby import strip_tashkeel, strip_diacritics
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import emoji

nltk.download('punkt_tab')
nltk.download('stopwords')
# def download_model_files():
#     github_files = [
#         "en/config.json",
#         "en/special_tokens_map.json",
#         "en/tokenizer_config.json",
#         "en/vocab.txt",
#         "ar/config.json",
#         "ar/special_tokens_map.json",
#         "ar/tokenizer_config.json",
#         "ar/vocab.txt",
#     ]
    
#     if not os.path.exists("model.safetensors"):
#         model_url_en = "https://drive.google.com/uc?id=1Q3WFKlNe12qXcwDnUmrrf6OkamwiXLG-"
#         model_url_ar = "https://drive.google.com/uc?id=1ig3la7xbgKI0Q9iz79b2_OD5cpf_Jx-X"

#         gdown.download(model_url_en, "model.safetensors", quiet=False)
st.set_page_config(page_title="YouTube Comments Sentiment Analysis", layout="wide")
st.title("ğŸ¥ YouTube Comments Sentiment Analysis")
st.markdown("---")
def download_model_files(language):
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©"""
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ©
    lang_code = "ar" if language == "Arabic" else "en"
    model_dir = f"models/{lang_code}"
    os.makedirs(model_dir, exist_ok=True)
    
    # Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®ÙÙŠÙØ© Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    light_files = ["config.json", "vocab.txt", "special_tokens_map.json", "tokenizer_config.json"]
    
    for filename in light_files:
        src = f"{lang_code}/{filename}"
        dst = f"{model_dir}/{filename}"
        
        if not os.path.exists(dst):
            try:
                with open(src, 'rb') as f_src, open(dst, 'wb') as f_dst:
                    f_dst.write(f_src.read())
            except Exception as e:
                st.error(f"Error copying {filename}: {str(e)}")
# https://drive.google.com/file/d/1dceNrR-xO-UclWEAZBCNC3YgzykdNnnH/view?usp=drive_link
    # ØªØ­Ù…ÙŠÙ„ model.safetensors Ù…Ù† Google Drive
    
    drive_links = {
        "ar": "https://drive.google.com/uc?id=1dceNrR-xO-UclWEAZBCNC3YgzykdNnnH",
        "en": "https://drive.google.com/uc?id=1Q3WFKlNe12qXcwDnUmrrf6OkamwiXLG-"
    }
    
    model_path = f"{model_dir}/model.safetensors"
    if not os.path.exists(model_path):
        try:
            gdown.download(drive_links[lang_code], model_path, quiet=False)
        except Exception as e:
            st.error(f"Error downloading model.safetensors: {str(e)}")

@st.cache_resource
def load_model(language):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ù„ÙŠ"""
    lang_code = "ar" if language == "Arabic" else "en"
    model_path = f"models/{lang_code}"
    
    download_model_files(language)
    
    try:
        tokenizer = BertTokenizer.from_pretrained(model_path)
        model = BertForSequenceClassification.from_pretrained(model_path)
        model.eval()
        return model, tokenizer
    except Exception as e:
        st.error(f"Error loading model: {str(e)}")
        return None, None

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù„ØºØ© ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.header("ğŸŒ Language Settings")
language = st.sidebar.radio(
    "Select Comment Language:",
    ("Arabic", "English"),
    index=0
)
arabert_prep = ArabertPreprocessor(model_name="models/ar")
def clean_arabic_text(text):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    text = re.sub(r'http\S+|www\S+|@\w+|#\w+', '', text)
    text = re.sub(r'[\U00010000-\U0010ffff]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ
    text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)  # Ø¥Ø¨Ù‚Ø§Ø¡ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
    
    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AraBERT
    text = arabert_prep.preprocess(text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = ' '.join(text.split())
    return text.strip()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
language_code = "arabic" if language == "Arabic" else "english"
model, tokenizer = load_model(language_code)

def predict_sentiment(text, language):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¹ØªØ¨Ø© Ù„Ù„Ù…Ø­Ø§ÙŠØ¯"""
    if language == "arabic":
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        text = normalize_arabic(text)
        
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding=True
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)[0]
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¹ØªØ¨Ø© Ù„Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø­Ø§ÙŠØ¯Ø©
            if probs[1] < 0.65:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø«Ù‚Ø© Ø§Ù„Ù…Ø­Ø§ÙŠØ¯ Ø£Ù‚Ù„ Ù…Ù† 65%
                final_pred = torch.argmax(probs * torch.tensor([1.2, 1.0, 1.2]))  # ØªÙ‚Ù„ÙŠÙ„ ÙˆØ²Ù† Ø§Ù„Ù…Ø­Ø§ÙŠØ¯
            else:
                final_pred = torch.argmax(probs)
            
            label_map = {0: "Ø³Ù„Ø¨ÙŠ", 1: "Ù…Ø­Ø§ÙŠØ¯", 2: "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"}
            colors = {0: "ğŸ”´", 1: "ğŸŸ¡", 2: "ğŸŸ¢"}
    else:
        # Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ (ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ)
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding=True
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)[0]
            final_pred = torch.argmax(probs)
            label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}
            colors = {0: "ğŸ”´", 1: "ğŸŸ¡", 2: "ğŸŸ¢"}
        
    return label_map[final_pred.item()], probs[final_pred].item(), colors[final_pred.item()]

# Ø¥Ø¶Ø§ÙØ© Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù…Ù† Notebook
def convert_emojis(text):
    text = emoji.demojize(text, language='en')
    emoji_translations = {
        "face_with_tears_of_joy": "Ø¶Ø­Ùƒ",
        "red_heart": "Ø­Ø¨",
        "angry_face": "ØºØ¶Ø¨",
        "crying_face": "Ø­Ø²Ù†",
        "smiling_face_with_smiling_eyes": "Ø³Ø¹Ø§Ø¯Ø©",
        "thumbs_up": "Ø§Ø¹Ø¬Ø§Ø¨",
        "clapping_hands": "ØªØµÙÙŠÙ‚",
        "fire": "Ø±Ø§Ø¦Ø¹",
        "ğŸ˜‚": "Ø¶Ø­Ùƒ", "â¤": "Ø­Ø¨", "ğŸ˜": "Ø­Ø¨",
        "ğŸ˜Š": "Ø³Ø¹Ø§Ø¯Ø©", "ğŸ‘": "Ù…ÙˆØ§ÙÙ‚Ø©", "ğŸ˜¢": "Ø­Ø²Ù†",
        "ğŸ‘": "ØªØµÙÙŠÙ‚", "ğŸ”¥": "Ø±Ø§Ø¦Ø¹", "ğŸ˜ ": "ØºØ¶Ø¨"
    }

    for emoji_code, arabic_word in emoji_translations.items():
        text = text.replace(f":{emoji_code}:", arabic_word)

    return text

def has_emoji(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Symbols & Pictographs
        u"\U0001F680-\U0001F6FF"  # Transport & Map
        u"\U0001F1E0-\U0001F1FF"  # Flags (iOS)
        u"\U00002500-\U00002BEF"  # Chinese/Japanese/Korean
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        "]+", flags=re.UNICODE)
    return bool(emoji_pattern.search(text))
arabic_stopwords = set(stopwords.words("arabic"))
keep_words = {'Ù„Ø§', 'Ù„Ù…', 'Ù„Ù†', 'Ù…Ø§', 'Ù…Ø´', 'Ù„ÙŠØ³', 'Ø¨Ø¯ÙˆÙ†', 'ØºÙŠØ±', 'Ø¥Ù†', 'Ø¥Ø°', 'Ø¥Ø°Ø§'}
custom_stopwords = arabic_stopwords - keep_words
def remove_custom_stopwords(tokens):
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚ÙÙŠØ© Ø§Ù„Ù…Ø®ØµØµØ©"""
    return [word for word in tokens if word not in custom_stopwords]
def normalize_arabic(text):
    if has_emoji(text):
        text = convert_emojis(text)
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)  # Remove non-Arabic
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub(r'Ù‰', 'ÙŠ', text)
    text = re.sub(r'Ø¤', 'Ø¡', text)
    text = re.sub(r'Ø¦', 'Ø¡', text)
    text = re.sub(r'Ø©', 'Ù‡', text)
    text = re.sub(r'\bÙ…Ø´\b', 'Ù„ÙŠØ³', text)
    text = re.sub(r'\bÙ…Ùˆ\b', 'Ù„ÙŠØ³', text)
    text = re.sub(r'\bÙ…Ø§ (\w+)', r'Ù„ÙŠØ³ \1', text)
    text = re.sub(r'\b(\w+)Ø´\b', r'\1', text)
    text = strip_tashkeel(text)
    text = strip_diacritics(text)
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)      # Remove digits
    text = re.sub(r'[a-zA-Z]', '', text) # Remove English
    text = re.sub(r'[^\u0621-\u064A]', ' ', text) # Keep Arabic only
    text = re.sub(r'[\u061F\u060C\u061B]', '', text)
    tokens = word_tokenize(text)
    tokens = remove_custom_stopwords(tokens)
    return ' '.join(tokens)

def extract_video_id(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
        r'youtube\.com\/watch\?.*v=([a-zA-Z0-9_-]{11})'
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

# def get_comments_without_api(video_url, max_comments=100):
#     """Ø¬Ù„Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… API"""
#     video_id = extract_video_id(video_url)
#     downloader = YoutubeCommentDownloader()
#     comments = []
#     try:
#         for comment in downloader.get_comments_from_url(f"https://www.youtube.com/watch?v={video_id}"):
#             comments.append({
#                 'author': comment['author'],
#                 'text': comment['text'],
#                 'likes': int(comment['votes']),
#                 'published': ''
#             })
#             if len(comments) >= max_comments:
#                 break
#     except Exception as e:
#         st.error(f"Error during scraping: {str(e)}")
#     return comments

def get_comments_without_api(video_url, max_comments=100):
    video_id = extract_video_id(video_url)
    downloader = YoutubeCommentDownloader()
    comments = []
    try:
        for comment in downloader.get_comments_from_url(f"https://www.youtube.com/watch?v={video_id}"):
            comments.append({
                'author': comment['author'],
                'text': comment['text'],
                'likes': int(comment['votes']),
                'published': ''  # Not available
            })
            if len(comments) >= max_comments:
                break
    except Exception as e:
        st.error(f"Error during scraping: {str(e)}")
    return comments

def get_youtube_comments(video_id, api_key=None, max_comments=100):
    """Fetch video comments from YouTube"""
    return get_comments_without_api(f"https://www.youtube.com/watch?v={video_id}", max_comments)

    # youtube = build('youtube', 'v3', developerKey=api_key)

    # comments = []
    # try:
    #     request = youtube.commentThreads().list(
    #         part='snippet',
    #         videoId=video_id,
    #         maxResults=min(max_comments, 100),
    #         order='relevance'
    #     )

    #     while request and len(comments) < max_comments:
    #         response = request.execute()

    #         for item in response['items']:
    #             comment = item['snippet']['topLevelComment']['snippet']
    #             comments.append({
    #                 'author': comment['authorDisplayName'],
    #                 'text': comment['textDisplay'],
    #                 'likes': comment['likeCount'],
    #                 'published': comment['publishedAt']
    #             })

    #         # Fetch more comments if available
    #         if 'nextPageToken' in response and len(comments) < max_comments:
    #             request = youtube.commentThreads().list(
    #                 part='snippet',
    #                 videoId=video_id,
    #                 pageToken=response['nextPageToken'],
    #                 maxResults=min(max_comments - len(comments), 100),
    #                 order='relevance'
    #             )
    #         else:
    #             break

    # except Exception as e:
    #     st.error(f"Error fetching comments: {str(e)}")
    #     return []

    # return comments

def analyze_comments(comments, language_code="english"):
    """Analyze sentiment of comments with language support"""
    if language_code == "arabic":
        label_map = {0: "Ø³Ù„Ø¨ÙŠ", 1: "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", 2: "Ù…Ø­Ø§ÙŠØ¯"}
    else:
        label_map = {0: "Negative", 1: "Positive", 2: "Neutral"}

    results = []
    for comment in comments:
        sentiment, confidence, emoji = predict_sentiment(comment['text'], language_code)
        results.append({
            'comment': comment['text'][:100] + "..." if len(comment['text']) > 100 else comment['text'],
            'author': comment['author'],
            'sentiment': sentiment,
            'confidence': confidence,
            'emoji': emoji,
            'likes': comment['likes']
        })
    return results

def create_visualizations(results, language):
    """Create visualizations"""
    df = pd.DataFrame(results)
    
    if language == "arabic":
        titles = {
            'pie': "ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            'bar': "Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            'hist': "ØªÙˆØ²ÙŠØ¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø©"
        }
        colors = {'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': '#2ecc71', 'Ø³Ù„Ø¨ÙŠ': '#e74c3c', 'Ù…Ø­Ø§ÙŠØ¯': '#f39c12'}
    else:
        titles = {
            'pie': "Sentiment Distribution",
            'bar': "Number of Comments by Sentiment",
            'hist': "Confidence Level Distribution"
        }
        colors = {'Positive': '#2ecc71', 'Negative': '#e74c3c', 'Neutral': '#f39c12'}
    
    # Calculate sentiment counts
    sentiment_counts = df['sentiment'].value_counts()
    
    # Pie chart
    fig_pie = px.pie(
        names=sentiment_counts.index,
        values=sentiment_counts.values,
        title=titles['pie'],
        color=sentiment_counts.index,
        color_discrete_map=colors
    )
    fig_pie.update_traces(textposition='inside', textinfo='percent+label')

    # Bar chart
    fig_bar = px.bar(
        x=sentiment_counts.index,
        y=sentiment_counts.values,
        title=titles['bar'],
        labels={'x': 'Sentiment', 'y': 'Number of Comments'},
        color=sentiment_counts.index,
        color_discrete_map=colors
    )

    # Confidence histogram
    fig_confidence = px.histogram(
        df,
        x='confidence',
        color='sentiment',
        title=titles['hist'],
        labels={'confidence': 'Confidence Level', 'count': 'Count'},
        color_discrete_map=colors
    )

    return fig_pie, fig_bar, fig_confidence, df
# App UI
st.set_page_config(page_title="YouTube Comments Sentiment Analysis", layout="wide")

test_samples = [
    ("Ø§Ù„ÙÙŠÙ„Ù… Ø±Ø§Ø¦Ø¹ ÙˆÙ…Ù…ØªØ¹", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"),
    ("Ø³ÙŠØ¡ Ø¬Ø¯Ø§Ù‹ ÙˆÙ„Ø§ Ø£Ù†ØµØ­ Ø¨Ù‡", "Ø³Ù„Ø¨ÙŠ"),
    ("Ø´Ø§Ù‡Ø¯Øª Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„Ø¨Ø§Ø±Ø­Ø©", "Ù…Ø­Ø§ÙŠØ¯")
]

for text, expected in test_samples:
    pred, conf, _ = predict_sentiment(text, "arabic")
    st.write(f"Ø§Ù„Ù†Øµ: {text} | Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {expected} | Ø§Ù„Ù†ØªÙŠØ¬Ø©: {pred} | Ø§Ù„Ø«Ù‚Ø©: {conf:.2%}")
# Sidebar
st.sidebar.header("âš™ï¸ Settings")

# API Key input
# api_key = st.sidebar.text_input(
#     "YouTube API Key:",
#     type="password",
#     help="You can obtain an API Key from Google Cloud Console"
# )

# Video URL input
video_url = st.sidebar.text_input(
    "YouTube Video URL:",
    placeholder="https://www.youtube.com/watch?v=..."
)

# Number of comments
max_comments = st.sidebar.slider("Number of comments to analyze:", 10, 500, 100)

# Analyze button
analyze_button = st.sidebar.button("ğŸ” Analyze Comments", type="primary")

# Single comment analysis
st.sidebar.markdown("---")
st.sidebar.header("ğŸ“ Single Comment Analysis")
single_comment = st.sidebar.text_area("Enter a comment to analyze:")

if st.sidebar.button("Analyze Comment"):
    if single_comment:
        sentiment_id, confidence = predict_sentiment(single_comment)
        label_map_ar = {0: "Negative", 1: "Positive", 2: "Neutral"}
        colors = {0: "ğŸ”´", 1: "ğŸŸ¢", 2: "ğŸŸ¡"}

        st.sidebar.markdown(f"**Result:** {colors[sentiment_id]} {label_map_ar[sentiment_id]}")
        st.sidebar.markdown(f"**Confidence Level:** {confidence:.2%}")
    else:
        st.sidebar.warning("Please enter a comment to analyze")

# Main content
if analyze_button:
    if not video_url:
        st.error("âš ï¸ Please enter the YouTube video URL")
    else:
        video_id = extract_video_id(video_url)
        if not video_id:
            st.error("âš ï¸ Invalid video URL")
        else:
            with st.spinner("ğŸ”„ Fetching and analyzing comments..."):
                comments = get_comments_without_api(video_url, max_comments)
                
                if not comments:
                    st.error("âŒ No comments found or an error occurred")
                else:
                    results = analyze_comments(comments, language_code)
                    fig_pie, fig_bar, fig_hist, df = create_visualizations(results, language_code)
                    
                    st.success(f"âœ… Successfully analyzed {len(results)} comments!")
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
                    col1, col2, col3, col4 = st.columns(4)
                    sentiment_counts = df['sentiment'].value_counts()
                    
                    with col1:
                        positive = sentiment_counts.get('Ø¥ÙŠØ¬Ø§Ø¨ÙŠ' if language_code == "arabic" else 'Positive', 0)
                        st.metric("Positive" if language == "English" else "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", 
                                 f"{positive} ({positive/len(results):.1%})")
                    
                    with col2:
                        negative = sentiment_counts.get('Ø³Ù„Ø¨ÙŠ' if language_code == "arabic" else 'Negative', 0)
                        st.metric("Negative" if language == "English" else "Ø³Ù„Ø¨ÙŠ", 
                                 f"{negative} ({negative/len(results):.1%})")
                    
                    with col3:
                        neutral = sentiment_counts.get('Ù…Ø­Ø§ÙŠØ¯' if language_code == "arabic" else 'Neutral', 0)
                        st.metric("Neutral" if language == "English" else "Ù…Ø­Ø§ÙŠØ¯", 
                                 f"{neutral} ({neutral/len(results):.1%})")
                    
                    with col4:
                        avg_conf = df['confidence'].mean()
                        st.metric("Avg. Confidence" if language == "English" else "Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©", 
                                 f"{avg_conf:.2%}")
                    
                    st.markdown("---")
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
                    col1, col2 = st.columns(2)
                    with col1:
                        st.plotly_chart(fig_pie, use_container_width=True)
                    with col2:
                        st.plotly_chart(fig_bar, use_container_width=True)
                    
                    st.plotly_chart(fig_hist, use_container_width=True)
                    
                    st.markdown("---")
                    st.subheader("ğŸ“‹ Comments Details")
                    
                    # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    filter_sentiment = st.selectbox(
                        "Filter by sentiment:" if language == "English" else "ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
                        ["All"] + list(df['sentiment'].unique())
                    )
                    if filter_sentiment != "All":
                        filtered_df = df[df['sentiment'] == filter_sentiment]
                    else:
                        filtered_df = df
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„
                    display_cols = ['author', 'comment', 'sentiment', 'confidence', 'likes']
                    display_df = filtered_df[display_cols].copy()
                    display_df.columns = ['Author', 'Comment', 'Sentiment', 'Confidence', 'Likes']
                    display_df['Confidence'] = display_df['Confidence'].apply(lambda x: f"{x:.2%}")
                    
                    st.dataframe(
                        display_df,
                        use_container_width=True,
                        hide_index=True
                    )
                    
                    # Ø²Ø± Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ Download Results (CSV)",
                        data=csv,
                        file_name=f"youtube_sentiment_{video_id}.csv",
                        mime="text/csv"
                    )
else:
    st.markdown("""
    ## ğŸ“Š Features:
    - Automatic sentiment analysis of YouTube comments
    - Support for both Arabic and English
    - Interactive visualizations
    - Detailed statistics
    - Filtering and CSV download
    - Single comment analysis
    """)