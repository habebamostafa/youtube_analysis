import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import re
from collections import Counter
from youtube_comment_downloader import YoutubeCommentDownloader
import gdown
import os
import shutil
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import emoji

# Download NLTK data (only once)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')

# ==============================================
# Helper Functions
# ==============================================

def has_emoji(text):
    """Check if text contains emoji"""
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Symbols & Pictographs
        u"\U0001F680-\U0001F6FF"  # Transport & Map
        u"\U0001F1E0-\U0001F1FF"  # Flags (iOS)
        u"\U00002500-\U00002BEF"  # Chinese/Japanese/Korean
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        "]+", flags=re.UNICODE)
    return bool(emoji_pattern.search(text))

def improved_convert_emojis(text):
    """Convert emojis to Arabic words with enhanced sentiment weighting"""
    text = emoji.demojize(text, language='en')
    
    # Enhanced Arabic emoji translations
    emoji_translations = {
        # Strong positive
        "face_with_tears_of_joy": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„",
        "red_heart": "Ø­Ø¨ Ø¹Ø´Ù‚ Ù…Ø­Ø¨Ø© Ø±Ø§Ø¦Ø¹",
        "smiling_face_with_smiling_eyes": "Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¨Ù‡Ø¬Ø© Ø¬Ù…ÙŠÙ„",
        "thumbs_up": "Ø¥Ø¹Ø¬Ø§Ø¨ Ù…ÙˆØ§ÙÙ‚Ø© Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø²",
        "clapping_hands": "ØªØµÙÙŠÙ‚ Ø¥Ø¹Ø¬Ø§Ø¨ Ø¨Ø±Ø§ÙÙˆ Ø±Ø§Ø¦Ø¹",
        "fire": "Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø² Ø¬Ù…ÙŠÙ„ ÙŠØ¬Ù†Ù†",
        "party_popper": "Ø§Ø­ØªÙØ§Ù„ ÙØ±Ø­ Ø³Ø¹Ø§Ø¯Ø© Ø±Ø§Ø¦Ø¹",
        "grinning_face": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„",
        "heart_eyes": "Ø­Ø¨ Ø¥Ø¹Ø¬Ø§Ø¨ Ø¬Ù…ÙŠÙ„ Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø²",
        "rolling_on_the_floor_laughing": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ù‚ÙˆÙŠ Ø¬Ù…ÙŠÙ„",
        "face_blowing_a_kiss": "Ø­Ø¨ Ù‚Ø¨Ù„Ø© Ø³Ø¹Ø§Ø¯Ø© Ø¬Ù…ÙŠÙ„",
        "smiling_face_with_heart-eyes": "Ø­Ø¨ Ø³Ø¹Ø§Ø¯Ø© Ø¬Ù…ÙŠÙ„ Ø±Ø§Ø¦Ø¹",
        
        # Negative
        "crying_face": "Ø­Ø²Ù† Ø¨ÙƒØ§Ø¡ Ø£Ø³Ù Ø³ÙŠØ¡",
        "angry_face": "ØºØ¶Ø¨ Ø²Ø¹Ù„ Ø³ÙŠØ¡",
        "broken_heart": "Ø­Ø²Ù† Ø£Ù„Ù… ÙØ±Ø§Ù‚ Ø³ÙŠØ¡",
        "disappointed_face": "Ø®ÙŠØ¨Ø© Ø£Ù…Ù„ Ø­Ø²Ù† Ø³ÙŠØ¡",
        "face_with_steam_from_nose": "ØºØ¶Ø¨ Ø²Ø¹Ù„ Ø³ÙŠØ¡",
        
        # Direct symbols
        "ğŸ˜‚": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„", "â¤": "Ø­Ø¨ Ù…Ø­Ø¨Ø© Ø±Ø§Ø¦Ø¹", "ğŸ˜": "Ø­Ø¨ Ø¥Ø¹Ø¬Ø§Ø¨ Ø¬Ù…ÙŠÙ„ Ø±Ø§Ø¦Ø¹",
        "ğŸ˜Š": "Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„", "ğŸ‘": "Ø¥Ø¹Ø¬Ø§Ø¨ Ù…ÙˆØ§ÙÙ‚Ø© Ø±Ø§Ø¦Ø¹", "ğŸ˜¢": "Ø­Ø²Ù† Ø¨ÙƒØ§Ø¡ Ø³ÙŠØ¡",
        "ğŸ‘": "ØªØµÙÙŠÙ‚ Ø¥Ø¹Ø¬Ø§Ø¨ Ø±Ø§Ø¦Ø¹", "ğŸ”¥": "Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø² Ø¬Ù…ÙŠÙ„ ÙŠØ¬Ù†Ù†", "ğŸ˜ ": "ØºØ¶Ø¨ Ø²Ø¹Ù„ Ø³ÙŠØ¡",
        "ğŸ‰": "ÙØ±Ø­ Ø§Ø­ØªÙØ§Ù„ Ø³Ø¹Ø§Ø¯Ø© Ø±Ø§Ø¦Ø¹", "ğŸ¥°": "Ø­Ø¨ Ø³Ø¹Ø§Ø¯Ø© Ø¬Ù…ÙŠÙ„", "ğŸ˜˜": "Ø­Ø¨ Ù‚Ø¨Ù„Ø© Ø¬Ù…ÙŠÙ„",
        "ğŸ¤£": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ù‚ÙˆÙŠ Ø¬Ù…ÙŠÙ„", "ğŸ’”": "Ø­Ø²Ù† Ø£Ù„Ù… ÙØ±Ø§Ù‚ Ø³ÙŠØ¡", "ğŸ˜": "Ø­Ø²Ù† Ø®ÙŠØ¨Ø© Ø£Ù…Ù„ Ø³ÙŠØ¡",
        "âœ¨": "Ø¬Ù…ÙŠÙ„ Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø²", "ğŸ’•": "Ø­Ø¨ Ù…Ø­Ø¨Ø© Ø¬Ù…ÙŠÙ„", "ğŸŒŸ": "Ø±Ø§Ø¦Ø¹ Ù…Ù…ØªØ§Ø² Ø¬Ù…ÙŠÙ„",
        "ğŸ˜ƒ": "Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„", "ğŸ˜„": "Ø³Ø¹Ø§Ø¯Ø© ÙØ±Ø­ Ø¬Ù…ÙŠÙ„", "ğŸ˜†": "Ø¶Ø­Ùƒ Ø³Ø¹Ø§Ø¯Ø© Ø¬Ù…ÙŠÙ„"
    }

    for emoji_code, arabic_words in emoji_translations.items():
        text = text.replace(f":{emoji_code}:", arabic_words)
    
    return text

def enhanced_normalize_arabic(text):
    """Enhanced Arabic text normalization preserving important words"""
    if has_emoji(text):
        text = improved_convert_emojis(text)
    
    # Important sentiment indicators to preserve
    positive_indicators = [
        'Ø±Ø§Ø¦Ø¹', 'Ø¬Ù…ÙŠÙ„', 'Ù…Ù…ØªØ§Ø²', 'Ø­Ù„Ùˆ', 'Ø¬Ø§Ù…Ø¯', 'ÙŠØ¬Ù†Ù†', 'Ø¹Ø¸ÙŠÙ…', 'Ù…Ø°Ù‡Ù„', 'ØªØ­ÙØ©',
        'Ø£Ø­Ø¨', 'Ø¨Ø­Ø¨', 'Ø­Ø¨', 'Ù…Ø­Ø¨Ø©', 'Ø§Ø¹Ø¬Ø§Ø¨', 'Ù…ÙˆØ§ÙÙ‚Ø©', 'Ø¨Ø±Ø§ÙÙˆ', 'ØªØµÙÙŠÙ‚',
        'Ø³Ø¹Ø§Ø¯Ø©', 'ÙØ±Ø­', 'Ø¨Ù‡Ø¬Ø©', 'Ø¶Ø­Ùƒ', 'Ù…Ø¨Ø³ÙˆØ·', 'Ù…Ù†Ø¨Ø³Ø·', 'Ø­Ø¨ÙŠØ¨', 'Ø­Ø¨ÙŠØ¨ØªÙŠ',
        'ÙŠØ­Ø¨ÙŠØ¨ØªÙŠ', 'Ø±ÙˆØ¹Ø©', 'Ø¨Ø¯ÙŠØ¹', 'Ù„Ø°ÙŠØ°', 'Ø§Ø³ØªÙ…Ø±ÙŠ', 'Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'Ø§Ù„Ù Ø´ÙƒØ±',
        'Ø§Ø­Ø³Ù†Øª', 'Ù…Ø¨Ø±ÙˆÙƒ', 'Ù…ÙˆÙÙ‚', 'Ù†Ø§Ø¬Ø­', 'Ø¬ÙŠØ¯', 'Ø­Ø³Ù†', 'Ù…Ù†ÙŠØ­', 'ÙƒÙˆÙŠØ³'
    ]
    
    negative_indicators = [
        'Ø³ÙŠØ¡', 'ÙˆØ­Ø´', 'Ù…Ø´ Ø­Ù„Ùˆ', 'Ù…Ø®Ø±Ø¨', 'ÙØ¸ÙŠØ¹', 'Ù‚Ø¨ÙŠØ­', 'ØºØ¨ÙŠ', 'Ø³Ø®ÙŠÙ',
        'Ø£ÙƒØ±Ù‡', 'ÙƒØ±Ù‡Øª', 'Ø²Ù‡Ù‚Øª', 'Ù…Ù„Ù„', 'Ø­Ø²Ù†', 'Ø²Ø¹Ù„', 'ØºØ¶Ø¨', 'Ø®ÙŠØ¨Ø©',
        'ØªØ¹Ø¨Ø§Ù†', 'Ù…Ù…Ù„Ù„', 'Ø¨Ø·Ø§Ù„', 'Ø²ÙØª', 'Ù…Ø­Ø¨Ø·', 'Ù…Ø³ØªØ§Ø¡', 'Ù…ØªØ¶Ø§ÙŠÙ‚'
    ]
    
    # Add sentiment indicators
    sentiment_boost = ""
    for word in positive_indicators:
        if word in text.lower():
            sentiment_boost += " Ø¥ÙŠØ¬Ø§Ø¨ÙŠ_Ù‚ÙˆÙŠ "
    
    for word in negative_indicators:
        if word in text.lower():
            sentiment_boost += " Ø³Ù„Ø¨ÙŠ_Ù‚ÙˆÙŠ "
    
    # Basic text normalization
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)  # Keep Arabic only
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)  # Unify alef
    text = re.sub(r'Ù‰', 'ÙŠ', text)  # Unify yeh
    text = re.sub(r'Ø¤', 'Ø¡', text)  # Unify hamza
    text = re.sub(r'Ø¦', 'Ø¡', text)  # Unify hamza
    text = re.sub(r'Ø©', 'Ù‡', text)  # Unify ta marbuta
    
    # Handle colloquial negation
    text = re.sub(r'\bÙ…Ø´\b', 'Ù„ÙŠØ³', text)
    text = re.sub(r'\bÙ…Ùˆ\b', 'Ù„ÙŠØ³', text)
    text = re.sub(r'\bÙ…Ø§\s+(\w+)', r'Ù„ÙŠØ³ \1', text)
    text = re.sub(r'\b(\w+)Ø´\b', r'\1', text)  # Remove negation "sh"
    
    # Remove numbers and English
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[a-zA-Z]', '', text)
    text = re.sub(r'[^\u0621-\u064A\s]', ' ', text)  # Arabic only
    
    # Combine processed text with sentiment indicators
    final_text = (text + " " + sentiment_boost).strip()
    
    return ' '.join(final_text.split())

def post_process_sentiment(text, sentiment, confidence):
    """Post-processing to improve sentiment accuracy"""
    original_text = text.lower()
    
    # Rules for religious content
    religious_keywords = ['Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ù†Ø¨ÙŠ', 'ØµÙ„Ù‰', 'Ø§Ø³ØªØºÙØ±', 'Ø¯Ø¹Ø§Ø¡', 'Ø±Ø¨', 'ÙŠØ§Ø±Ø¨', 'Ø§Ù„Ù„Ù‡Ù…', 'Ø³Ø¨Ø­Ø§Ù†', 'Ø§Ù„Ø­Ù…Ø¯']
    if any(word in original_text for word in religious_keywords):
        if sentiment == "Ø³Ù„Ø¨ÙŠ":
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.80)
        elif sentiment == "Ù…Ø­Ø§ÙŠØ¯":
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.75)
    
    # Rules for strong positive emojis
    strong_positive_emojis = ['ğŸ‰', 'â¤', 'ğŸ˜', 'ğŸ¥°', 'ğŸ”¥', 'ğŸ‘', 'ğŸ˜‚', 'ğŸ¤£', 'âœ¨', 'ğŸ’•', 'ğŸ˜˜', 'ğŸ˜Š']
    emoji_count = sum([original_text.count(emoji) for emoji in strong_positive_emojis])
    if emoji_count >= 2:
        if sentiment == "Ù…Ø­Ø§ÙŠØ¯":
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.85)
        elif sentiment == "Ø³Ù„Ø¨ÙŠ" and emoji_count >= 3:
            return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.80)
    
    # Rules for strong positive words
    strong_positive_words = ['Ø±Ø§Ø¦Ø¹', 'Ø¬Ù…ÙŠÙ„', 'Ù…Ù…ØªØ§Ø²', 'Ø±ÙˆØ¹Ø©', 'ÙŠØ¬Ù†Ù†', 'Ø£Ø­Ø¨', 'Ø¨Ø­Ø¨', 'Ø¬Ø§Ù…Ø¯', 'ØªØ­ÙØ©', 'Ø­Ø¨ÙŠØ¨ØªÙŠ']
    positive_count = sum([1 for word in strong_positive_words if word in original_text])
    if positive_count >= 1 and sentiment == "Ù…Ø­Ø§ÙŠØ¯":
        return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.75)
    
    # Rules for strong negative words
    strong_negative_words = ['Ù…Ø®Ø±Ø¨', 'ÙˆØ­Ø´', 'Ø³ÙŠØ¡', 'ÙØ¸ÙŠØ¹', 'Ø£ÙƒØ±Ù‡', 'ÙƒØ±Ù‡Øª', 'Ù‚Ø¨ÙŠØ­', 'ØºØ¨ÙŠ']
    if any(word in original_text for word in strong_negative_words):
        if sentiment == "Ù…Ø­Ø§ÙŠØ¯":
            return "Ø³Ù„Ø¨ÙŠ", max(confidence, 0.80)
        elif sentiment == "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" and confidence < 0.7:
            return "Ø³Ù„Ø¨ÙŠ", max(confidence, 0.75)
    
    # Rules for simple questions
    question_patterns = ['Ø§ÙŠØ´ Ø§Ø³Ù…', 'Ù…ÙŠÙ†', 'ÙˆÙŠÙ†', 'ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ù…Ù…ÙƒÙ† Ø§Ø¹Ø±Ù', 'ÙƒÙ… Ø­Ù„Ù‚Ø©', 'Ø´Ùˆ Ø§Ø³Ù…']
    if any(pattern in original_text for pattern in question_patterns):
        return "Ù…Ø­Ø§ÙŠØ¯", max(confidence, 0.70)
    
    # Rules for thanks and appreciation
    thanks_patterns = ['Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'Ø§Ù„Ù Ø´ÙƒØ±', 'ÙŠØ¹Ø·ÙŠÙƒ', 'Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡', 'Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡']
    if any(pattern in original_text for pattern in thanks_patterns):
        return "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", max(confidence, 0.85)
    
    # Fallback for very low confidence
    if confidence < 0.4:
        return get_enhanced_fallback_sentiment(text, "arabic")[:2]
    
    return sentiment, confidence

def get_enhanced_fallback_sentiment(text, language):
    """Enhanced fallback sentiment analysis with expanded rules"""
    text_lower = text.lower()
    
    # Expanded word lists
    positive_words = {
        'ar': ['Ø±Ø§Ø¦Ø¹', 'Ø¬Ù…ÙŠÙ„', 'Ù…Ù…ØªØ§Ø²', 'Ø±ÙˆØ¹Ø©', 'ÙŠØ¬Ù†Ù†', 'Ø£Ø­Ø¨', 'Ø¨Ø­Ø¨', 'Ø¬Ø§Ù…Ø¯', 'ØªØ­ÙØ©', 'Ø­Ø¨ÙŠØ¨ØªÙŠ',
               'Ø³Ø¹Ø§Ø¯Ø©', 'ÙØ±Ø­', 'Ø¨Ù‡Ø¬Ø©', 'Ø¶Ø­Ùƒ', 'Ù…Ø¨Ø³ÙˆØ·', 'Ù…Ù†Ø¨Ø³Ø·', 'Ø­Ø¨ÙŠØ¨', 'Ø±ÙˆØ¹Ø©', 'Ø¨Ø¯ÙŠØ¹', 'Ù„Ø°ÙŠØ°',
               'Ø§Ø³ØªÙ…Ø±ÙŠ', 'Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±', 'Ø§Ù„Ù Ø´ÙƒØ±', 'Ø§Ø­Ø³Ù†Øª', 'Ù…Ø¨Ø±ÙˆÙƒ', 'Ù…ÙˆÙÙ‚', 'Ù†Ø§Ø¬Ø­', 'Ø¬ÙŠØ¯', 'Ø­Ø³Ù†'],
        'en': ['good', 'great', 'awesome', 'amazing', 'excellent', 'love', 'like', 'best', 
               'wonderful', 'fantastic', 'perfect', 'nice', 'beautiful', 'happy']
    }
    
    negative_words = {
        'ar': ['Ø³ÙŠØ¡', 'ÙˆØ­Ø´', 'Ù…Ø´ Ø­Ù„Ùˆ', 'Ù…Ø®Ø±Ø¨', 'ÙØ¸ÙŠØ¹', 'Ù‚Ø¨ÙŠØ­', 'ØºØ¨ÙŠ', 'Ø³Ø®ÙŠÙ',
               'Ø£ÙƒØ±Ù‡', 'ÙƒØ±Ù‡Øª', 'Ø²Ù‡Ù‚Øª', 'Ù…Ù„Ù„', 'Ø­Ø²Ù†', 'Ø²Ø¹Ù„', 'ØºØ¶Ø¨', 'Ø®ÙŠØ¨Ø©',
               'ØªØ¹Ø¨Ø§Ù†', 'Ù…Ù…Ù„Ù„', 'Ø¨Ø·Ø§Ù„', 'Ø²ÙØª', 'Ù…Ø­Ø¨Ø·', 'Ù…Ø³ØªØ§Ø¡', 'Ù…ØªØ¶Ø§ÙŠÙ‚'],
        'en': ['bad', 'terrible', 'awful', 'hate', 'worst', 'horrible', 'disgusting',
               'stupid', 'ugly', 'boring', 'annoying', 'angry', 'sad']
    }
    
    lang_code = 'ar' if language == 'arabic' else 'en'
    
    pos_count = sum(1 for word in positive_words[lang_code] if word in text_lower)
    neg_count = sum(1 for word in negative_words[lang_code] if word in text_lower)
    
    # Emoji analysis
    emoji_sentiment = 0
    if has_emoji(text):
        converted = improved_convert_emojis(text)
        pos_emoji = sum(1 for word in positive_words[lang_code] if word in converted)
        neg_emoji = sum(1 for word in negative_words[lang_code] if word in converted)
        emoji_sentiment = pos_emoji - neg_emoji
    
    # Calculate total sentiment
    total_sentiment = (pos_count - neg_count) + emoji_sentiment
    
    if total_sentiment > 0:
        return ("Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" if lang_code == 'ar' else "Positive"), 0.75, "ğŸŸ¢"
    elif total_sentiment < 0:
        return ("Ø³Ù„Ø¨ÙŠ" if lang_code == 'ar' else "Negative"), 0.75, "ğŸ”´"
    else:
        return ("Ù…Ø­Ø§ÙŠØ¯" if lang_code == 'ar' else "Neutral"), 0.5, "ğŸŸ¡"

def improved_predict_sentiment(text, language, model, tokenizer):
    """Enhanced sentiment prediction with better Arabic handling"""
    if not text.strip():
        return "Ù…Ø­Ø§ÙŠØ¯" if language == "arabic" else "Neutral", 0.5, "ğŸŸ¡"
    
    original_text = text
    
    # Text preprocessing
    if language.lower() == "arabic":
        processed_text = enhanced_normalize_arabic(text)
    else:
        processed_text = text.strip()
    
    try:
        # Model prediction attempt
        if len(processed_text) > 500:
            processed_text = processed_text[:500]
        
        model_vocab_size = model.config.vocab_size if hasattr(model.config, 'vocab_size') else 30000
        
        inputs = tokenizer(
            processed_text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=512,
            add_special_tokens=True,
            return_attention_mask=True
        )
        
        # Filter out-of-vocabulary tokens
        input_ids = inputs['input_ids'][0]
        unk_token_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else 1
        valid_mask = input_ids < model_vocab_size
        filtered_input_ids = torch.where(valid_mask, input_ids, torch.tensor(unk_token_id))
        inputs['input_ids'] = filtered_input_ids.unsqueeze(0)
        
        if torch.any(inputs['input_ids'] >= model_vocab_size):
            return get_enhanced_fallback_sentiment(original_text, language)
        
        # Model prediction
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probabilities = torch.nn.functional.softmax(logits, dim=1)[0]
            predicted_class = torch.argmax(logits, dim=1).item()
            confidence = probabilities[predicted_class].item()
        
        # Label mapping
        if hasattr(model.config, 'id2label') and model.config.id2label:
            model_label = None
            if predicted_class in model.config.id2label:
                model_label = model.config.id2label[predicted_class]
            elif str(predicted_class) in model.config.id2label:
                model_label = model.config.id2label[str(predicted_class)]
            
            label_normalization = {
                "LABEL_0": "Negative", "LABEL_1": "Positive", "LABEL_2": "Neutral",
                "0": "Negative", "1": "Neutral", "2": "Positive"
            }
            
            if model_label in label_normalization:
                model_label = label_normalization[model_label]
            
            if model_label:
                if language == "arabic":
                    label_mapping = {
                        "Negative": "Ø³Ù„Ø¨ÙŠ", "Positive": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Neutral": "Ù…Ø­Ø§ÙŠØ¯"
                    }
                    sentiment_label = label_mapping.get(model_label, model_label)
                else:
                    sentiment_label = model_label
                
                color_mapping = {
                    "Negative": "ğŸ”´", "Ø³Ù„Ø¨ÙŠ": "ğŸ”´",
                    "Positive": "ğŸŸ¢", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ": "ğŸŸ¢", 
                    "Neutral": "ğŸŸ¡", "Ù…Ø­Ø§ÙŠØ¯": "ğŸŸ¡"
                }
                color = color_mapping.get(sentiment_label, "âšª")
            else:
                raise ValueError("Could not find model label")
        else:
            # Fallback labels
            if language == "arabic":
                labels = ["Ø³Ù„Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"]
                colors = ["ğŸ”´", "ğŸŸ¡", "ğŸŸ¢"]
            else:
                labels = ["Negative", "Neutral", "Positive"]
                colors = ["ğŸ”´", "ğŸŸ¡", "ğŸŸ¢"]
            
            if predicted_class >= len(labels):
                return get_enhanced_fallback_sentiment(original_text, language)
            
            sentiment_label = labels[predicted_class]
            color = colors[predicted_class]
        
        # Post-processing for better accuracy
        final_sentiment, final_confidence = post_process_sentiment(
            original_text, sentiment_label, confidence
        )
        
        return final_sentiment, final_confidence, color
            
    except Exception as e:
        return get_enhanced_fallback_sentiment(original_text, language)

# ==============================================
# Model Download and Loading
# ==============================================

def download_model_files(language):
    """Download model files from Google Drive"""
    lang_code = "ar" if language == "Arabic" else "en"
    model_dir = f"models/{lang_code}"
    os.makedirs(model_dir, exist_ok=True)
    
    config_files = ["config.json", "vocab.txt", "special_tokens_map.json", "tokenizer_config.json"]
    
    for filename in config_files:
        src_path = f"{lang_code}/{filename}"
        dst_path = f"{model_dir}/{filename}"
        
        if not os.path.exists(dst_path):
            try:
                shutil.copyfile(src_path, dst_path)
            except Exception as e:
                st.error(f"Error copying {filename}: {str(e)}")

    model_files = {
        "ar": {
            "url": "https://drive.google.com/uc?id=1dceNrR-xO-UclWEAZBCNC3YgzykdNnnH",
            "dest": f"{model_dir}/model.safetensors"
        },
        "en": {
            "url": "https://drive.google.com/uc?id=1Q3WFKlNe12qXcwDnUmrrf6OkamwiXLG-",
            "dest": f"{model_dir}/model.safetensors"
        }
    }
    
    if not os.path.exists(model_files[lang_code]["dest"]):
        try:
            gdown.download(model_files[lang_code]["url"], model_files[lang_code]["dest"], quiet=False)
        except Exception as e:
            st.error(f"Download error: {str(e)}")

@st.cache_resource
def load_model(language):
    """Load the sentiment analysis model"""
    lang_code = "ar" if language == "Arabic" else "en"
    model_path = f"models/{lang_code}"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        
        model.resize_token_embeddings(len(tokenizer))
        model.eval()
        return model, tokenizer
    except Exception as e:
        st.error(f"Model loading failed: {str(e)}")
        return None, None

# ==============================================
# YouTube Comment Functions
# ==============================================

def extract_video_id(url):
    """Extract YouTube video ID from URL"""
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
        r'youtube\.com\/watch\?.*v=([a-zA-Z0-9_-]{11})'
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

def get_comments_without_api(video_url, max_comments=100):
    """Fetch YouTube comments without API"""
    video_id = extract_video_id(video_url)
    downloader = YoutubeCommentDownloader()
    comments = []
    try:
        for comment in downloader.get_comments_from_url(f"https://www.youtube.com/watch?v={video_id}"):
            comments.append({
                'author': comment['author'],
                'text': comment['text'],
                'likes': int(comment['votes']),
                'published': ''  # Not available
            })
            if len(comments) >= max_comments:
                break
    except Exception as e:
        st.error(f"Error during scraping: {str(e)}")
    return comments

def analyze_comments(comments, language_code, model, tokenizer):
    """Analyze sentiment of comments"""
    results = []
    for comment in comments:
        sentiment, confidence, emoji = improved_predict_sentiment(
            comment['text'], 
            language_code,
            model,
            tokenizer
        )
        results.append({
            'comment': comment['text'][:100] + "..." if len(comment['text']) > 100 else comment['text'],
            'author': comment['author'],
            'sentiment': sentiment,
            'confidence': confidence,
            'emoji': emoji,
            'likes': comment['likes']
        })
    return results

def create_visualizations(results, language):
    """Create sentiment visualizations"""
    df = pd.DataFrame(results)
    
    if language == "Arabic":
        titles = {
            'pie': "ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            'bar': "Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            'hist': "ØªÙˆØ²ÙŠØ¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø©"
        }
        colors = {'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': '#2ecc71', 'Ø³Ù„Ø¨ÙŠ': '#e74c3c', 'Ù…Ø­Ø§ÙŠØ¯': '#f39c12'}
    else:
        titles = {
            'pie': "Sentiment Distribution",
            'bar': "Number of Comments by Sentiment",
            'hist': "Confidence Level Distribution"
        }
        colors = {'Positive': '#2ecc71', 'Negative': '#e74c3c', 'Neutral': '#f39c12'}
    
    # Calculate sentiment counts
    sentiment_counts = df['sentiment'].value_counts()
    
    # Pie chart
    fig_pie = px.pie(
        names=sentiment_counts.index,
        values=sentiment_counts.values,
        title=titles['pie'],
        color=sentiment_counts.index,
        color_discrete_map=colors
    )
    fig_pie.update_traces(textposition='inside', textinfo='percent+label')

    # Bar chart
    fig_bar = px.bar(
        x=sentiment_counts.index,
        y=sentiment_counts.values,
        title=titles['bar'],
        labels={'x': 'Sentiment', 'y': 'Number of Comments'},
        color=sentiment_counts.index,
        color_discrete_map=colors
    )

    # Confidence histogram
    fig_confidence = px.histogram(
        df,
        x='confidence',
        color='sentiment',
        title=titles['hist'],
        labels={'confidence': 'Confidence Level', 'count': 'Count'},
        color_discrete_map=colors
    )

    return fig_pie, fig_bar, fig_confidence, df

# ==============================================
# Streamlit UI
# ==============================================

def main():
    st.set_page_config(page_title="YouTube Comments Sentiment Analysis", layout="wide")
    st.title("ğŸ¥ YouTube Comments Sentiment Analysis")
    st.markdown("---")
    
    # Download models
    download_model_files("English")
    download_model_files("Arabic")
    
    # Language selection
    st.sidebar.header("ğŸŒ Language Settings")
    language = st.sidebar.radio(
        "Select Comment Language:",
        ("Arabic", "English"),
        index=0
    )
    language_code = "arabic" if language == "Arabic" else "english"
    
    # Load model
    model, tokenizer = load_model(language) 
    if model is None or tokenizer is None:
        st.error("Failed to load model - please check the error messages")
        st.stop()
    
    # Video URL input
    st.sidebar.header("âš™ï¸ Settings")
    video_url = st.sidebar.text_input(
        "YouTube Video URL:",
        placeholder="https://www.youtube.com/watch?v=..."
    )
    
    # Number of comments
    max_comments = st.sidebar.slider("Number of comments to analyze:", 10, 500, 100)
    
    # Analyze button
    analyze_button = st.sidebar.button("ğŸ” Analyze Comments", type="primary")
    
    # Single comment analysis
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“ Single Comment Analysis")
    single_comment = st.sidebar.text_area("Enter a comment to analyze:")
    
    if st.sidebar.button("Analyze Comment"):
        if single_comment:
            sentiment, confidence, emoji_icon = improved_predict_sentiment(
                single_comment, 
                language_code,
                model,
                tokenizer
            )
            st.sidebar.markdown(f"**Result:** {emoji_icon} {sentiment}")
            st.sidebar.markdown(f"**Confidence:** {confidence:.2%}")
        else:
            st.sidebar.warning("Please enter a comment to analyze")
    
    # Main content
    if analyze_button:
        if not video_url:
            st.error("âš ï¸ Please enter the YouTube video URL")
        else:
            video_id = extract_video_id(video_url)
            if not video_id:
                st.error("âš ï¸ Invalid video URL")
            else:
                with st.spinner("ğŸ”„ Fetching and analyzing comments..."):
                    comments = get_comments_without_api(video_url, max_comments)
                    
                    if not comments:
                        st.error("âŒ No comments found or an error occurred")
                    else:
                        results = analyze_comments(comments, language_code, model, tokenizer)
                        fig_pie, fig_bar, fig_hist, df = create_visualizations(results, language)
                        
                        st.success(f"âœ… Successfully analyzed {len(results)} comments!")
                        
                        # Display quick stats
                        col1, col2, col3, col4 = st.columns(4)
                        sentiment_counts = df['sentiment'].value_counts()
                        
                        # Get labels based on language
                        if language == "Arabic":
                            pos_label, neg_label, neu_label = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ø³Ù„Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯"
                            pos_text, neg_text, neu_text, conf_text = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ø³Ù„Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯", "Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©"
                        else:
                            pos_label, neg_label, neu_label = "Positive", "Negative", "Neutral"
                            pos_text, neg_text, neu_text, conf_text = "Positive", "Negative", "Neutral", "Avg. Confidence"
                        
                        with col1:
                            positive = sentiment_counts.get(pos_label, 0)
                            st.metric(pos_text, f"{positive} ({positive/len(results):.1%})")
                        
                        with col2:
                            negative = sentiment_counts.get(neg_label, 0)
                            st.metric(neg_text, f"{negative} ({negative/len(results):.1%})")
                        
                        with col3:
                            neutral = sentiment_counts.get(neu_label, 0)
                            st.metric(neu_text, f"{neutral} ({neutral/len(results):.1%})")
                        
                        with col4:
                            avg_conf = df['confidence'].mean()
                            st.metric(conf_text, f"{avg_conf:.2%}")
                        
                        st.markdown("---")
                        
                        # Display charts
                        col1, col2 = st.columns(2)
                        with col1:
                            st.plotly_chart(fig_pie, use_container_width=True)
                        with col2:
                            st.plotly_chart(fig_bar, use_container_width=True)
                        
                        st.plotly_chart(fig_hist, use_container_width=True)
                        
                        st.markdown("---")
                        st.subheader("ğŸ“‹ Comments Details")
                        
                        # Filter options
                        if language == "Arabic":
                            filter_label = "ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:"
                            all_option = "Ø§Ù„ÙƒÙ„"
                        else:
                            filter_label = "Filter by sentiment:"
                            all_option = "All"
                            
                        filter_sentiment = st.selectbox(
                            filter_label,
                            [all_option] + list(df['sentiment'].unique())
                        )
                        
                        if filter_sentiment != all_option:
                            filtered_df = df[df['sentiment'] == filter_sentiment]
                        else:
                            filtered_df = df
                        
                        # Display results table
                        display_cols = ['author', 'comment', 'sentiment', 'confidence', 'likes']
                        display_df = filtered_df[display_cols].copy()
                        display_df.columns = ['Author', 'Comment', 'Sentiment', 'Confidence', 'Likes']
                        display_df['Confidence'] = display_df['Confidence'].apply(lambda x: f"{x:.2%}")
                        
                        st.dataframe(
                            display_df,
                            use_container_width=True,
                            hide_index=True
                        )
                        
                        # Download button
                        csv = df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="ğŸ“¥ Download Results (CSV)",
                            data=csv,
                            file_name=f"youtube_sentiment_{video_id}.csv",
                            mime="text/csv"
                        )
    else:
        # Display instructions
        if language == "Arabic":
            st.markdown("""
            ## ğŸ“Š Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
            - ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø´Ø§Ø¹Ø± ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨
            - Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØªÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            - Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© ØªÙØ§Ø¹Ù„ÙŠØ©
            - Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©
            - ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªÙ†Ø²ÙŠÙ„Ù‡Ø§ Ø¨ØµÙŠØºØ© CSV
            - ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©
            
            ## ğŸš€ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
            1. Ø§Ø®ØªØ± Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø© (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
            2. Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
            3. Ø§Ø®ØªØ± Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡Ø§
            4. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª"
            5. Ø§Ø³ØªØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆÙ‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„Ù‡Ø§ Ø¥Ø°Ø§ Ø±ØºØ¨Øª
            """)
        else:
            st.markdown("""
            ## ğŸ“Š Features:
            - Automatic sentiment analysis of YouTube comments
            - Support for both Arabic and English
            - Interactive visualizations
            - Detailed statistics
            - Filtering and CSV download
            - Single comment analysis
            
            ## ğŸš€ How to Use:
            1. Select your preferred language (Arabic/English)
            2. Enter a YouTube video URL in the sidebar
            3. Choose the number of comments to analyze
            4. Click "Analyze Comments"
            5. View results and download CSV if needed
            """)

if __name__ == "__main__":
    main()
